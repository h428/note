

# 1. urllib

- urllib.request 中的 urlopen 用于打开一个链接，得到网页的源码

## 1.1 利用 urlopen 打开网页并使用正则提取 title

- 使用 find_all 方法进行匹配

```py
from urllib.request import urlopen
import re

html = urlopen("http://lemengfun.com/read/nv26ce16f06e1f83ca/2712712").read().decode('utf-8')

# print(html)

# r 表示非前缀，内部可以不必转义，前一个 .+? 匹配属性，后一个 .+? 为匹配 title 元素内容
# ?: 表示非捕获组，需要可控编号时使用
res = re.findall(r"<title(?:.+?)>(.+?)</title>", html)
print("Page title is: ", res[0])
```

- 使用 match 方法进行

```py
from urllib.request import urlopen
import re

html = urlopen("http://lemengfun.com/read/nv26ce16f06e1f83ca/2712712").read().decode('utf-8')

# print(html)

# r 表示非前缀，内部可以不必转义，前一个 .+? 匹配属性，后一个 .+? 为匹配 title 元素内容
# ?: 表示非捕获组，需要可控编号时使用
reg = r"[\s\S]*<title(.+?)>(.+?)</title>[\s\S]*"
res = re.match(reg, html)
print("Page title is: ", res.group(0))  # 0 匹配到网页全内容
print("Page title is: ", res.group(1))  # 1 匹配到第一个捕获组，即属性
print("Page title is: ", res.group(2))  # 2 匹配到第二个捕获组，即 title 内容
```

## 1.2 利用正则提取网页的所有 p 元素的内容以及 a 元素的 href 属性

```py
from urllib.request import urlopen
import re

html = urlopen("file:///C:/Users/Administrator/Desktop/test.html").read().decode('utf-8')  # html 文件见结尾

print(html)

# 匹配出所有 p 标签中的内容，其中 re.DOTALL 让 . 能匹配 \n，或者可以用 [\s\S]
# results = re.findall(r'<p(?:.*?)>(.*?)</p>', html, flags=re.DOTALL)
results = re.findall(r'<p(?:.*?)>([\s\S]*?)</p>', html)  # 用 [\s\S] 代替 re.DOTALL
print(results)

# 取出所有 a 标签的 href 属性
results = re.findall(r'<a.*?href="(.*?)"(?:.*?)>(?:.*?)</a>', html, flags=re.DOTALL)
print(results)

# res = re.match(r"")

```

# 2. BeautifulSoup

- 在前面的例子中，我们使用正则表达式来匹配以提取网页的内容，而 BeautifulSoup 则可以简化提取网页内容的过程，且功能更加强大
- BeautifulSoup 在包装网页源码后，主要通过 find, find_all 两个方法搜索对应的元素，当然搜索时可以带条件
- find 方法查找满足条件的第一个元素，其返回结果是一个 Tag 对象，表示一个元素节点，可以读取节点的属性，值等内容，还可以在元素内部进一步调用 find, find_all 在该节点内部进行搜索
- find_all 方法查找所有满足条件的元素，其返回结果是一个 ResultSet 对象（即 Tag 对象的列表），可以迭代它得到一个个 Tag 对象，然后在 Tag 对象内部进行 find, find_all


## 2.1 读取所有 p, a 元素并打印它们的节点值、href 属性等

- 样例 1 : 读取所有节点的值以及属性
```py
from bs4 import BeautifulSoup
from urllib.request import urlopen

html = urlopen("file:///C:/Users/Administrator/Desktop/test.html").read().decode('utf-8')

# 使用 BeautifulSoup 包装读取的网页
soup = BeautifulSoup(html, features='lxml')

all_p = soup.find_all('p')  # 找出所有 p 节点
print([l.string for l in all_p])  # 打印所有 p 的节点值

all_a = soup.find_all('a')  # 找出所有 a 节点
print([l['href'] for l in all_a])  # 打印所有 a 的 href 值
```

## 2.2 使用 class 属性搜索 div 并在 div 内部进一步搜索 p 元素

```py
from bs4 import BeautifulSoup
from urllib.request import urlopen

html = urlopen("file:///C:/Users/Administrator/Desktop/test.html").read().decode('utf-8')

# 使用 BeautifulSoup 包装读取的网页
soup = BeautifulSoup(html, features='lxml')

all_p = soup.find_all('div', {'class': 'tt'})  # 找出所有 class=tt 的 p 节点
print([l.text for l in all_p])


div = soup.find('div', {'class': 'wrap-content'})  # 找到一个 div，是一个 Tag 对象
all_p = div.find_all('p', {'class': 'pp'})  # 在 div 搜索所有 class=pp 的 p 标签
print([l.text for l in all_p])  # 打印所有 p 的节点值


divs = soup.find_all('div', {'class': 'wrap-content'})  # find_all 返回的是 ResultSet，是 Tag 的集合
for div in divs:
    all_p = div.find_all('p', {'class': 'pp'})
    print([l.text for l in all_p])  # 打印所有 p 的节点值

```

## 2.3 配合正则的搜索 : 利用正则过滤 a 元素的 href 属性来搜索目标节点

```py
from bs4 import BeautifulSoup
from urllib.request import urlopen
import re

html = urlopen("file:///C:/Users/Administrator/Desktop/test.html").read().decode('utf-8')

# 使用 BeautifulSoup 包装读取的网页
soup = BeautifulSoup(html, features='lxml')

links = soup.find_all('a', {'href': re.compile(r'.*?\.com')})  # 利用正则找到 href 为 com 结尾的 a
print([l.text for l in links])

```

## 2.4 百度百科爬取样例：从网页爬虫页面开始一个个往下搜索满足条件的百度百科页面

```py
from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import random
import util

base_url = "https://baike.baidu.com"  # 前缀
his = ["/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB"]  # 起始页面：网页爬虫

for i in range(20):  # 迭代 20 次
    url = base_url + his[-1]  # 取出 url

    # 打开对应网页
    html = urlopen(url).read().decode('utf-8')

    # 使用 BeautifulSoup 包装读取的网页
    soup = BeautifulSoup(html, features='lxml')

    # 打印页面的 h1 标题以及打印对应 url
    print(i, soup.find("h1").text, 'url : ', his[-1])

    # 找到下一个合法的 url，返回的是 ResultSet 对象（Tag 对象的集合）
    sub_urls = soup.find_all("a", {
        "target": "_blank",
        "href": re.compile("^/item/(%.{2})+")
    })

    # 找到则加入 his 以用于下次迭代
    if len(sub_urls) != 0:
        his.append(random.sample(sub_urls, 1)[0]['href'])  # random.sample 取出指定长度的片段
    else:
        continue  # 没找到则继续使用原来的链接

```


# 3. 多功能的 Requests

- 很多页面使用 get 请求就可以完成，但部分页面可能还涉及 post, head, delete, put 等 HTTP 方法，使用 requests 模块可以模拟这些多功能的 HTTP 请求
- 当然最常见的还是 get, post，这两个基本已经覆盖了 95% 的请求

## 3.1 使用 Requests 测试 get, post 请求

```py
from bs4 import BeautifulSoup
import requests
import webbrowser
import util

# 测试 get 请求
url = r"http://localhost:5000/manage/product/list"  # 请求 url
params = {"pageNum": 1, "pageSize": 3}  # 请求参数
res = requests.get(url, params=params)  # get 请求使用 params 传递参数
print(res.text)

# 测试 post 请求
url = r"http://localhost:5000/login"  # 请求 url
params = {"username": "admin", "password": "admin"}  # 请求参数
res = requests.post(url, data=params)  # post 请求使用 data 传递参数
print(res.text)

# 测试上传文件
url = r"http://localhost:5000/manage/img/upload"  # 请求 url
file = {"image": open(r"E:\tmp\img\mi6_3.jpg", "rb")}  # 待上传的文件
res = requests.post(url, files=file)  # 上传文件
print(res.text)

```

## 3.2 


# 8. 实践

- 清明时节爱上我

```py
from bs4 import BeautifulSoup
import requests
import webbrowser
import util
import os

prefix = r"http://www.lemengfun.com/"
titles = []
articles = []

# 测试 get 请求
url = r"http://www.lemengfun.com/bookDetail/nv26ce16f06e1f83ca"  # 请求 url

headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3",
    "Accept-Encoding": "gzip, deflate",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "max-age=0",
    "Connection": "keep-alive",
    "Host": "www.lemengfun.com",
    "If-None-Match": '"4b92-NRr8S6z7hYKM6JdgCpqSkDYLCwc"',
    # "Referer": "https://www.google.com/",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36"
}

html = requests.get(url, headers=headers)

# print(html.text)

soup = BeautifulSoup(html.text, features='lxml')

menus = soup.find("div", {"class": "menus"})
a_all = menus.find_all("a")
href_all = [l["href"] for l in a_all]

for i, href in enumerate(href_all):
    html = requests.get(prefix + href, headers=headers)
    soup = BeautifulSoup(html.text, features='lxml')

    title = soup.find("h1", {"class": "title"})
    titles.append(title.text)

    article = soup.find("div", {"class": "con"})
    articles.append(article.text.strip('" '))

print(len(titles))
print(len(articles))
with open('novel.txt', 'w', encoding='utf-8') as f:
    for i in range(len(titles)):
        f.write(titles[i])
        f.write("\n\n")
        f.write(articles[i])
        f.write("\n\n\n\n")


```

# 9. 补充


```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>abalone</title>
    <script src="https://cdn.bootcss.com/jquery/1.12.3/jquery.js"></script>
    <!--<script src="js/jquery.js"></script>-->
</head>
<body>


<div id="d1" class="one two three">
    <a href="http://www.baidu.com" target="_blank">百度</a>
    <a href="http://www.bilibili.com">bilibili</a>
    <p>明月几时有，把酒问青天</p>
    <p class="ttt" id="aaa" href="http://www.hao.com">不知天上宫阙，今夕是何年</p>
</div>
<div class="wrap">
    <div class="wrap-content">
        <p class="pp" id="pax1">
            我欲乘风归去，又恐琼楼玉宇，高处不胜寒
        </p>
        <p class="pp" id="pax2">起舞弄清影，何似在人间</p>
    </div>
    <p>
        转朱阁，低绮户，照无眠
    </p>
    <p class="tt">不应有恨，何事长向别时圆</p>
    <p class="tt">
        人有悲欢离合，月有阴晴圆缺，此事古难全
    </p>
</div>
<p class="oo">但愿人长久，千里共婵娟</p>
<div contenteditable>123</div>
<div id="divA">This is <span>some</span> text</div>

<script>

    var div = document.getElementById("d1");
    var divA = document.getElementById("divA");
    // var p = document.getElementById("p1");


</script>

</body>

</html>
```