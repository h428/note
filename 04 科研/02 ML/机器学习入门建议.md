
# 数学知识

可能涉及的数学知识，但不要先去学数学，而是遇到什么再学什么：
- 线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等
- 统计与概率：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等
- 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的求解方法如梯度下降、牛顿法、基因算法和模拟退火等
- 微积分：偏微分，链式法则，矩阵求导等
- 信息论、数值理论等


# 经典书籍

- Machine Learning in action（机器学习实践）：适合入门
- 周志华老师的西瓜书《机器学习》
- 李航老师的《统计学习方法》
- 《Patten Recognition and Machine Learning》（贝叶斯学派）
- 《Elements of Statistical Learning》（频率学派）

# 其他经典资料

- Andrew Ng 在coursera上机器学习教程（建立对ML的直觉和兴趣，熟悉matlab或者octave）

# 不那么枯燥的学习路线

1. Andrew Ng在coursera上机器学习教程（主要是建立对ML的直觉和兴趣，熟悉matlab或者octave，当你每次提交作业成功，一定会增加对ML的喜爱，一定要耐着性子过一遍甚至是几面这个课程）
2. 《机器学习实战》(Python写的)和李航老师的《统计学习方法》。方法：对照着看，同步进行，先看后者的理论，再在前者的帮助下自己实现算法。这样既不会浪费太多时间自己想算法，同时也通过代码让自己对算法理解更深入。很多东西不自己实现是体会不到的。另外，前者里面虽然有一些错误，但是代码组织还是比较考究的。后者则是李航老师的佳作，算法的描述忠诚于原作的论文，例子非常丰富而且接地气，看起来比较舒服畅快。
3. 选一个自己喜欢的方向，找相关的论文。或者自己实现自己想法，因为机器学习涉及的面非常广，很难在短时间同时精通所有的技术。所以最好的办法是有一定基础过后，围绕一个自己感兴趣的方向开展学习，当你对一个方向深入过后，再回过头来看其他的方法，就会有一种高屋建瓴的感觉（因为机器学习算法是扎根于统计学的，所以很多地方有相通之处）。另外，论文建议先找CV或者NLP相关的，NIPS之类的ML专门杂志理论性比较强，可能会有些困难。
