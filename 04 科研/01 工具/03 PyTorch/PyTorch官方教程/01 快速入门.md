
# 0. 安装教程

- 本教程参考[官方教程](https://pytorch.org/tutorials/)
- 相关安装命令可以访问 [官网](https://pytorch.org/get-started/locally/)
- 我在 Win10 Anaconda 下主要步骤为大致为：
```bash
conda create –n torch python=3.5 # 创建环境
activate torch # 进入环境
conda install pytorch -c pytorch  # 该指令会变化，请参考官网
pip install torchvision  # torchvision
```
- 测试是否成功安装：
```py
from __future__ import print_function
import torch
x = torch.rand(5, 3)
print(x)
# 快速验证 GPU 环境是否可用
torch.cuda.is_available() # GPU 环境是否可用
torch.cuda.device_count() # 返回可用的 GPU 数量
torch.cuda.get_device_name(0) # 返回gpu名字，设备索引默认从0开始
torch.cuda.current_device() # 返回当前使用的 GPU 索引，默认为 0
# 测试代码
dev = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
x = torch.rand(3, 5, device = dev)
print(x.device)
```

# 1. PyTorch 基础

- PyTorch 是 NumPy 的替代品，并可以使用 GPU 的强大功能
- 深度学习研究平台，提供最大的灵活性和速度

## 1.1 开始使用

- 导入需要用到的包

```py
from __future__ import print_function
import torch
import numpy as np
```

**Tensors**

- Tensors 和 numpy 中的 ndarrays 较为相似, 与此同时 Tensor 也能够使用 GPU 来加速运算。
- 构造 Tensor 的方式（和 numpy 大致相似）：
```py
# 构造 Tensor 
x = torch.empty(5, 3)  # 构造一个未初始化的5*3的Tensor，矩阵元素为值很小的数
x = torch.rand(5, 3)  # 构造一个随机初始化的Tensor
x = torch.zeros(5, 3, dtype=torch.long)  # 零矩阵，并制定类型
x = torch.tensor([5.5, 3])  # 直接从数据构造 Tensor
x = x.new_ones(5, 3, dtype=torch.double)  # 创建全为一的 Tensor
x = torch.randn_like(x, dtype=torch.float)  # 从已有 Tensor 创建等规模的 Tensor，内容随机
print(x.size())  # x.size()返回 Tensor 规模，返回 torch.Size 类型，其实际上是一个 tuple
```
- torch 定义了常见的几种 dtype，在使用 CPU/GPU 训练时，他们分别有对应的 CPU/GPU Tensor，下面只列举四个最常见的 dtype，其他请参考[官方文档](https://pytorch.org/docs/stable/tensors.html) :
    - torch.float(=torch.float32), torch.double(=torch.float64)
    - torch.int(=torch.int32), torch.long(=torch.int64)

**Operations**

```py
# Tensor 运算
x = torch.rand(5, 3)
y = torch.rand(5, 3)

# 加法
# print(x + y)  # 执行加法，不改变 x
# print(torch.add(x, y))  # 执行加法，不改变 x

# 加法与赋值
result = torch.empty(5, 3)
torch.add(x, y, out=result)  # 执行 x+y，将结果存储到 result
# print(result)

y.add_(x)  # 把 x 加到 y，会改变 y，类似的带 _ 的运算都会改变本身的值
# print(y)


# 切片操作，和 Python 保持一致
print("切片操作：", x[:, 1])


# reshape 相关，使用 torch.view 函数
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # -1 所在的维会根据其他维自动计算，只能有一个 -1
print(x.size(), y.size(), z.size())

# 若只有一个元素，可直接使用 .item() 取出
x = torch.randn(1)
# print(x)
# print(x.item())
```

## 1.2 NumPy 桥：Tensor 和 ndarray 之间的相互转换

- ndarray 和 Tensor 可以互相转换，a.numpy() 可将 Tensor 转换为 ndarray，而 b.from_numpy(...) 可以从 numpy 转换为 Tensor
- 注意 Torch 的 Tensor 和 numpy 的 array 会共享他们的存储空间，修改一个会导致另外的一个也被修改
```py
# Tensor 和 ndarray 的互相转换
a = torch.ones(5)  # 生成 torch.Tensor 全一矩阵

# 使用 .numpy() 将 torch.Tensor 转换为 numpy.ndarray
# 注意 Torch 的 Tensor 和 numpy 的 array 会共享他们的存储空间，修改一个会导致另外的一个也被修改，如果需要不共享可以使用 np.array() 从转化后的内容重新创建一份
b = a.numpy()  # 转换为 numpy.ndarray 类型
a.add_(1)  # a 中的所有值 + 1
# print("全一 Tensor 加一的结果：", a)  # 打印 a
# print("a 改变会导致 b 也发生改变：", b)  # 打印 b 会发现 b 也发生改变

# 使用 torch.from_numpy 将 ndarray 转换为 Tensor
a = np.ones(5)  # 创建全为一的 numpy.ndarray
b = torch.from_numpy(a)  # 转换为 torch.Tensor，会共享内存，如果需要不共享可以直接使用 torch.tensor 直接从 ndarray 创建
np.add(a, 1, out=a)  # a + 1 并将结果存储到 a
# print("全一 np.ndarray 加一的结果：", a)  # 打印 a
# print("a 改变会导致 b 也发生改变：", b)  # 打印 b 会发现 b 也发生改变
```

## 1.3 CUDA Tensor：GPU加速运算

- 另外除了 CharTensor 之外，所有的 tensor 都可以在 CPU 运算和 GPU 运算之间相互转换
- 样例代码：
```py
# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device("cuda")          # CUDA 设备对象
    y = torch.ones_like(x, device=device)  # 直接再 GPU 上创建 Tensor
    x = x.to(device)                       # 或直接转换为 GPU ``.to("cuda")``
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!
```
- GPU 相关方法
```py
flag = torch.cuda.is_available() # GPU 环境是否可用
cnt = torch.cuda.device_count() # 返回可用的 GPU 数量
name = torch.cuda.get_device_name(0) # 返回gpu名字，设备索引默认从0开始
idx = torch.cuda.current_device() # 返回当前使用的 GPU 索引，默认为 0
torch.cuda.set_device(1) # 设置全局使用的 GPU
with torch.cuda.device(1): # 在该 with 环境中临时使用对应的 GPU，在 with 内部 current_device() 得到 1，结束后又会变回原来的值
    torch.cuda.current_device()
```
- 若不考虑到多 GPU 并行，推荐的做法是，在代码开头统一获取并定义要运行的 device，之后的代码全都基于此 device 创建数据，这样可以达到快速更改运行设备的目的
- 旧的 GPU, CPU 数据互转的方法
```py
x = x.cpu()  # 另一种转化到 cpu 上的方法（应该是比较旧的方法，建议使用 to 配合 device 使用）
x = x.cuda() # 另一种转换到 gpu 上的方法，默认使用第一张显卡，但 to 是更加通用的方法，可以
```

# 2. 自动求导相关

- autograd 是 PyTorch 中神经网络的核心包
- autograd 会自动对涉及的 Tensor 进行反向传播，求导数
- Pytorch 是一个运行时的框架，这意味着您的 backprop 由您的代码运行方式定义，每次迭代都可以不同。

## 2.1 Tensors

- torch.Tensor 是求导的核心类型，将 Tensor 的 .requires_grad 属性设置为 True，则框架会跟踪在该 Tensor 上的所有操作,当调用 .backward() 后，所有的相关 Tensor 的导数都会被计算，并存储在 .grad 中
- Tensor 即使在计算得值后也会存储计算历史，即保存了计算图来源，想阻止 Tensor 追踪计算图，可以使用 .detach() 方法将其从原有计算图中分离出来，并防止将来的计算被跟踪（这样无法求导了）
- 为了避免追踪历史和使用内存，可以将代码编写在 with torch.no_grad(): 的代码快中，这在执行模型评估时十分有效，因为模型中含有大量可训练参数，这些 Tensor 的 requires_grad=True，但我们在执行评估是并不需要跟踪计算图和求导，因此可用该代码取消跟踪计算图
- 另一个和求导相关的类：Function
- Tensor 和 Function 互相连接并组成一个计算图，这张计算图也可以称作前面提到的计算历史
- 每个 Tensor 含有一个 .grad_fn 属性，其指向一个 Function，这个 Function 即创建的当前的 Tensor（框架反向传播求导时会用到）
- 用户创建的 Tensor，grad_fn 为 None
- 使用 Tensor 的 .backward() 计算当前及之前所有可导节点的导数，一般执行反向传播的对象为一个 scale，若是向量，则要提供一个等规模的 gradient（作为系数）
- 一般来说， loss 就是一个 scale
- 除了在创建 Tensor 时提供 requires_grad 外，还可以调用 requires_grad_ 函数进行更改，如：
```py
a = torch.randn(2, 2)  # 随机Tensor,requires_grad=False
a = ((a * 3) / (a - 1))  # Operation
print(a.requires_grad)  # requires_grad=False
a.requires_grad_(True)  # 修改 requires_grad=True
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)  # 打印 Operation 指向的函数
```

## 2.2 Gradients

- 计算图构建完成后，对于最终的 loss，调用 .backward() 方法进行反向传播，计算梯度
- 在反向传播完成后，模型只会保留叶子节点的梯度，叶子节点指的是非 Operation 产生节点，其是用户自己创建的 Tensor，一般是神经网络参数
- 而 Operation 节点一般含有 grad_fn 属性，指向了创建该 Tensor 的一个 Function，框架在反向传播求导时会用到
- 样例代码：
```py
x = torch.ones(2, 2, requires_grad=True)
y = x + 2
print(y.grad_fn)  # y 是作为一个 operation 创建的，因此它有 grad_fn.
print(y.requires_grad)
z = y * y * 3
out = z.mean()
out.backward()
print(x.grad)  # 取得叶子节点的梯度，叶子节点指的是非Operation节点，用户自己创建的Tensor，一般是神经网络参数
print(y.grad)  # None, y 是一个 Operation，而非叶子结点，其 grad 默认不保存，可以使用 hook 函数提取
```
- 也可以求多个变量的导数，但此时需要传递等规模的系数
```py
x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() < 1000:
    y = y * 2

print(y)

gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(gradients)  #  gradients 必须和 y 规模一致，且会作为系数传给y

print(x.grad)
```
- 可以在代码块 with torch.no_grad(): 取消跟踪计算图，这在进行模型评估很常用，因为评估不需要跟踪计算图并求导：
```py
x = torch.ones(2, 2, requires_grad=True)
print(x.requires_grad)  # True
print((x ** 2).requires_grad)  # True

with torch.no_grad():
    # 在该代码块内部，默认 requires_grad=False，无法求导，常用于模型评估
    print((x ** 2).requires_grad)  # False
```

# 3. 神经网络

- 神经网络基于 torch.nn 包，可使用该包下的函数构建神经网络
- nn 包依赖于前面提到的 autograd 包，用于对模型进行自动求导
- 类 nn.Module 包含各个层，以及一个 forward(input) 方法，该方法返回 output
- 传统的训练神经网络的程序大致包含下述步骤：
    - 定义包含权重的神经网络
    - 迭代输入或数据集
    - 处理输入，并提供给神经网络
    - 计算 loss
    - 反向传播，求得各个权重的偏导数
    - 更新网络的权重
- 可能涉及的包：
```py
import torch
import torch.nn as nn
import torch.nn.functional as F
```

## 3.1 定义网络

- PyTorch 中，自定义模型都继承于 nn.Module，你只需要定义 forward 函数，框架会自动实现 backward 函数
- 定义网络的样例代码：
```py
class Net(nn.Module):  # 自定义网络，继承于 nn.Module

    def __init__(self):
        """
        构造函数，主要用于初始化各个网络层
        """
        super(Net, self).__init__()
        # 1个输入通道，6个输出通道，卷积核大小为5x5
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 线性变换: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        """
        前向传播，将init中定义的各个层组装起来
        """
        # 卷积层1：先 relu，后 2×2池化
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # 卷积层2：再次 relu，然后后 2×2池化
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        # 展开为全连接层
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # 注意最后一层不要 relu，因为 loss 函数中自己会加上激活函数
        x = self.fc3(x)  
        return x

    def num_flat_features(self, x):
        """
        计算单个样本的特征数，以将x展开为向量
        """
        # 取出x的各个维度（除了第0维的batch_size以外）
        size = x.size()[1:] 
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

# 创建网络对象
net = Net()
# 打印网络结构
print(net)
"""
Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
"""
```
- 你只需自定义 forward 函数，框架会自动帮你实现 backward 函数执行求导，在 forward 中，你可以使用任意在 Tensor 上可以执行的 Operation
- 通过 net.parameters() 获取模型的权重
```py
params = list(net.parameters())
print(len(params))
print(params[0].size())  # 卷积层1的权重参数
print(params[1].size())  # 卷积层1的偏置
# 打印各层的参数，注意偏置单独存储在一个元素中
for step, p in enumerate(params):
    print(step, p.size())
```
- 对于我们构建的网络，我们随机初始化一个 32x32 的输入执行以下测试（注意 32x32适用于 CIFAR10，若是 MNIST 需要将 28x28 放大到 32x32 执行测试）
```py
input = torch.randn(1, 1, 32, 32)  # 随机初始化输入 (1, 1, 32, 32) -> (n, c, w, h)
out = net(input)  # 执行测试
print(out)  # 打印结果
```
- 之后便可以对 out 模拟一次反向传播：
```py
input = torch.randn(1, 1, 32, 32)  # 随机初始化输入 (1, 1, 32, 32) -> (n, c, w, h)
out = net(input)  # 执行测试
print(out)  # 打印结果
```
- 需要注意， torch.nn 只支持带 batch_size 的样本，而不支持单个样本
- 比如，nn.Conv2d 接受的输入是 4 维张量：nSamples x nChannels x Height x Width
- 若是只有一个样本（batch_size=1），需要执行 input.unsqueeze(0) 添加出第 0 维，而不能只传递不带 batch_size 的输入给模型
```py
x = torch.randn(1, 32, 32)
print(x.size())  # [1, 32, 32]
print(a.unsqueeze(0).size())  # [1, 1, 32, 32]
```

**复习一下前面所学**

- torch.Tensor : 多维数组，支持通过 backward() 进行自动求导，并保存关于 Tensor 的导数
- nn.Module : 神经网络模型，封装了参数，提供了方便的方法，并可以很容易转换到 GPU 上进行运算
- nn.Parameter : 是一种 Tensor，当指定了模型的属性时，会自动左外参数注册到参数中
- autograd.Function : 实现自动求导的前向和反向的定义，每一个 Operation 类型的 Tensor 至少一个 Function 表名创建该 Tensor 的运算类型，以在反向求导时使用并进行自动根据类型求导
- 前面已经学习如何定义一个网络、处理输入、以及调用反向传播
- 接下来我们将学习定义损失函数、更新网络参数


## 3.2 损失函数

- 一个损失函数以 (output, target) 作为输入，并计算一个实数值表名预测值和真实标签之间的差距
- nn 包中提供了多种不同的损失函数，如最简单的均方差损失 nn.MSELoss
```py
output = net(input)  # 模型的输出
target = torch.randn(10)  # 虚假的真实标签或真实值
target = target.view(1, -1)  # 调整输出形状，确保和输出同个形状
criterion = nn.MSELoss()  # 定义损失函数对象

loss = criterion(output, target)  # 计算本次损失
print(loss)  # 打印本次损失值
```
- 如果你直接反向跟踪 loss 的计算图，同个使用 .grad_fn 属性，你可以看到一个和下面反序的计算图：
```
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
```
- 因此若我们调用 loss.backward()，则整个计算图会计算关于 loss 的导数，且所有前向的 requires_grad=True 的 Tensor 的导数会得到计算并存储在 .grad 中（只会存储叶子节点，非叶子节点计算后就清空了）
- 下面是反向跟踪计算图的例子：
```py
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
```

## 3.3 反向传播

- 使用 loss.backward() 执行反传播
- 在执行之前要用 net.zero_grad() 清空已经存在的梯度，否则梯度会累加（这是由多节点反向传播到前一层同一个节点时所产生的累加，是链式求导规则所决定的，因此每次都要清零），下面为样例代码：
```py
net.zero_grad()     # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

## 3.4 更新权重

- 求得偏导数后，则可以使用梯度下降更新参数了
- 如随机梯度下降 SGD 公式为：weight = weight - learning_rate * gradient
- 可以直接使用 Python 代码执行更新：
```py
learning_rate = 0.01  # 学习速率
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```
- 但使用神经网络时，往往要用到不同的优化模型，此时可以使用优化器更新权重 torch.optim 包中提供了各种优化器，包括 SGD, Nesterov-SGD, Adam, RMSProp 等等
```py
import torch.optim as optim

# 创建优化器，同时提供参数
optimizer = optim.SGD(net.parameters(), lr=0.01)

# 在你的训练循环中执行下述代码
optimizer.zero_grad()   # 清空梯度缓存
output = net(input)  # 计算模型输出
loss = criterion(output, target)  # 计算预测和真实值的loss
loss.backward()  # 反向传播
optimizer.step()    # 利用优化器更新参数
```

# 4. 训练分类器

## 4.1 关于数据

- 通常，你都需要处理图片、文本、音频或视频数据，你可以使用 Python 标准包将数据载入到 ndarray 中，然后转化为 Tensor
- 对于图片，常见工具包有 Pillow, OpenCV
- 对于音频，常见工具包有 scipy, librosa
- 对于文本，原始 Python 类型即可，或 NLTK, SpaCy
- PyTorch 额外提供了 torchvision 包专门用于视觉领域的常见数据集处理，比如 Imagenet, CIFAR10, MNIST 等数据集，并提供了这些数据集的处理器(transformers)，最终返回 torchvision.datasets 和 torch.utils.data.DataLoader
- 这提供了极大的便利并避免编写样板代码
- 本教程，我们会使用 CIFAR10 作为数据集，该数据集包含10个类： ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. 每个样本的大小为 3x32x32，3 为通道，32x32为像素大小

## 4.2 训练图片分类器

训练一个分类器，大致包含如下步骤：

1. 使用 torchvision 载入和标准化 CIFAR10 中的训练集和测试集
2. 定义卷积神经网络
3. 定义损失函数和优化器
4. 在训练集上训练网络
5. 在测试集上测试网络模型

### 1. 载入和标准化CIFAR10数据集

- 使用 torchvision 中预定义的类，可以十分快速的载入数据集
- 先导入工具包
```py
import torch
import torchvision
import torchvision.transforms as transforms
```
- torch.utils.data 包下的 Dataset 和 DataLoader 是关于数据读取的两个核心类，其中 Dataset 是一个抽象类，自定义数据集需要继承该类，复写方法读取自定义数据集，详细可参考数据读取和处理教程
- torchvision.datasets.CIFAR10 继承于 Dataset，相当于为你写好了关于CIFAR10数据的数据读取方式，可以查看这类是否是 Dataset 子类：
```py
print(type(trainset))
print(isinstance(trainset, torch.utils.data.Dataset))  # True
```
- 一般创建 Dataset 是，往往还会经过一系列的 transform，即数据预处理过程，在定义 Dataset 时通过参数 transform 指明需要经过的预处理序列
- 利用 torchvision.datasets.CIFAR10 类读取 CIFAR10 数据集的代码如下：
```py
# 定义数据预处理序列
transform = transforms.Compose(
    [transforms.ToTensor(),  # 预定义的转换为 Tensor 的预处理类
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 预定义的标准化类

# 读取训练集，通过 transform 指定需要经过的预处理序列
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=False, transform=transform)
# 转化为 DataLoader，batch_size表示批处理量，shuffle表示是否随机打乱数据，num_workers为读取数据的线程数量
# 有时候，win下的多线程会报错，此时可尝试将num_workers修改为0查看情况
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

# 读取测试集
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=False, transform=transform)
# 转化为 DataLoader，一般测试集不随机打乱
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

# 数据集中预定义的类型
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
"""
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
Files already downloaded and verified
"""
```

**可视化一些训练样本**

```py
import matplotlib.pyplot as plt
import numpy as np

# 打印图片的函数
def imshow(img):
    img = img / 2 + 0.5     # 反标准化
    npimg = img.numpy()  # 拿到 ndarray，才可以用 plt 打印
    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # 注意 Tensor 中通道在第0维，而打印时通道要放第2维

# 随机取到一些图片
dataiter = iter(trainloader)
images, labels = dataiter.next()

# 打印图片
imshow(torchvision.utils.make_grid(images))
plt.show()
# 打印对应标签
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

### 2. 定义卷积神经网络

```py
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):  # 定义模型，继承于 nn.Module
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)  # 卷积层1：输入通道3，输出通道6
        self.pool = nn.MaxPool2d(2, 2)  # 池化层
        self.conv2 = nn.Conv2d(6, 16, 5)  # 卷积层：输入通道6，输出通道16
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 全连接层
        self.fc2 = nn.Linear(120, 84)  # 全连接层
        self.fc3 = nn.Linear(84, 10)  # 全连接层

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 创建模型
net = Net()
```

### 3. 定义损失函数和优化器

```py
import torch.optim as optim

criterion = nn.CrossEntropyLoss()  # 定义损失函数为交叉熵
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 定义优化器，并提供超参数
```

### 4. 训练网络

- 训练即在训练集上迭代，将每个batch喂给网络和优化器，然后前向传播、计算损失、反向传播、更新参数
```py
for epoch in range(2):  # 迭代次数 epoch_nums

    running_loss = 0.0  # 统计 loss ，没两千次输出一次平均 loss
    for i, data in enumerate(trainloader, 0):  # 索引从0开始（默认也是从0开始）
        # get the inputs
        inputs, labels = data  # 拿到单个样本

        # 清空梯度参数
        optimizer.zero_grad()

        # 前向 + 反向 + 更新
        outputs = net(inputs)  # 前向
        loss = criterion(outputs, labels)  # 计算 loss
        loss.backward()  # 反向
        optimizer.step()  # 更新

        # 打印损失：每两千个batch打印一次平均损失
        running_loss += loss.item()  # 累加
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
```

### 5. 在测试集上测试网络模型

- 我们前面在训练集上训练了两个 epoch，我们想查看网络训练的效果
- 测试集前面已经读取过，我们可以可视化几个测试集：
```py
dataiter = iter(testloader)  # 转换为迭代器
images, labels = dataiter.next()  # 拿到一个 batch 数据

# 打印
imshow(torchvision.utils.make_grid(images))
plt.show()
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))  # 打印类别
```
- 执行预测
```py
outputs = net(images)  # 执行前向传播进行预测，得到 4*10，outputs，是一个 softmax 概率
_, predicted = torch.max(outputs, 1)  # 按行求最大值和最大值下标，最大值下标即为类别，
# predicted = torch.argmax(outputs, 1)  # 可直接使用 torch.argmax 返回最大值下标

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
                              for j in range(4)))  # 打印预测类别

```
- 预测时计算预测准确率
```py
correct = 0
total = 0
with torch.no_grad():  # 预测时建议在 with torch.no_grad() 内部进行，避免跟踪计算图
    for data in testloader:
        images, labels = data  # 取出一个 batch，注意labels是类型，不是one-hot
        outputs = net(images)  # 对该batch执行前向传播
        _, predicted = torch.max(outputs.data, 1)  # 计算该 batch 的预测值
        total += labels.size(0)  #  总的增加样本数
        correct += (predicted == labels).sum().item()  # 计算预测正确数
print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))  # 计算准确率
```
- 此外，我们还可以统计，对于不同类别，该网络的准确率
```py
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data  # 取出batch
        outputs = net(images)  # 前向传播
        _, predicted = torch.max(outputs, 1)  # 计算预测值
        c = (predicted == labels).squeeze()  # 判断预测正确个数
        for i in range(4):  # 对batch中的每个样本
            label = labels[i]  # 该样本类别
            class_correct[label] += c[i].item()  # 该类别预测正确数 + 1或0（True 或 False）
            class_total[label] += 1  # 该类别总数 + 1

# 打印出所有类别的正确率
for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
```

## 4.3 在 GPU 上训练

- 就像将 Tensor 转换到 GPU 上一样，可以直接将模型转换到 GPU 上，并在 GPU 上的模型参数基础上定义优化器
```py
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Assume that we are on a CUDA machine, then this should print a CUDA device:
print(device)  # cuda:0
# 将模型和优化器转换到 GPU 上
net.to(device)
# 优化器有用到模型参数，因此要重新定义，否则还是CPU类型
criterion = nn.CrossEntropyLoss()  # 定义损失函数为交叉熵
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 定义优化器，并提供超参数
print(net)
```
- 由于前面定义的网络很小，且 batch_size 很小，在 GPU 上训练实际上并不会加速，并且可能由于花费了额外的拷贝时间而导致更慢，但对一个大的网络来说，GPU 的训练速度是远远高于 CPU 的
- 对于每个 batch 的数据也要转换到 GPU 模型上，然后在 GPU 上训练
```py

for epoch in range(2):  # 迭代次数 epoch_nums

    running_loss = 0.0  # 统计 loss ，没两千次输出一次平均 loss
    for i, data in enumerate(trainloader, 0):  # 索引从0开始（默认也是从0开始）
        # get the inputs
        inputs, labels = data  # 拿到单个样本
        # 同时将输入和标签转换到 GPU
        inputs, labels = inputs.to(device), labels.to(device)
        # 清空梯度参数
        optimizer.zero_grad()

        # 前向 + 反向 + 更新
        outputs = net(inputs)  # 前向
        loss = criterion(outputs, labels)  # 计算 loss
        loss.backward()  # 反向
        optimizer.step()  # 更新

        # 打印损失：每两千个batch打印一次平均损失
        running_loss += loss.item()  # 累加
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
```
- 需要注意，如果模型在 GPU 上，则训练、测试数据都需要在 GPU 上，否则无法执行，若训练完毕，可以将模型转换为 CPU 然后执行测试


## 4.4 在多个 GPU 上训练

- 参考教程[OPTIONAL: DATA PARALLELISM](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)
- 本文的 5

## 4.5 更多的 Tutorial

- Train neural nets to play video games
- Train a state-of-the-art ResNet network on imagenet
- Train a face generator using Generative Adversarial Networks
- Train a word-level language model using Recurrent LSTM networks
- More examples
- More tutorials
- Discuss PyTorch on the Forums
- Chat with other users on Slack

# 5. 可选：数据并行

- 下面，将讲述如何使用 DataParallel 类进行多个 GPU 并行计算
- 在 PyTorch 中，使用 GPU 很简单，只需要将模型转化到 GPU 上即可
```py
device = torch.device("cuda:0")
model.to(device)
```
- 之后，对于计算所需数据，也可调用 Tensor 的方法转化到 GPU 上：
```py
mytensor = my_tensor.to(device)
```
- 需要注意，对于 Tensor 的 to 方法，返回的是在 GPU 上的拷贝，并不更改原有 Tensor，因此需要将返回值赋值给一个新的 Tensor 变量
- 在多个 GPU 上执行前向、反向传播是很自然的事情，但 PyTorch 默认只使用一个 GPU，但通过 DataParallel，将很容易使用多个 GPU 进行并行计算，下面我们将探讨它：
```py
model = nn.DataParallel(model)
```

- 等有多个 GPU 再继续写...

## 5.1 导包和设置超参数

## 5.2 伪造数据集

## 5.3 简单模型

## 5.4 创建模型和 DataParallel
