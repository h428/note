
# 0. 综述

- 保存和加载模型主要涉及 3 个核心函数：
- torch.save : 将模型的序列对象存储到磁盘，它调用 Python 的 pickle 工具包执行序列化模型、张量和其他模型中所有的对象
- torch.load : 使用 pickle 的反序列化来讲对象从文件中读取到内存
- torch.nn.Module.load_state_dict : 将模型的状态字典 state_dict 反序列化到模型中

# 1. 什么是 state_dict

- 在 PyTorch 中，模型的可学习参数（权重和偏置）被放置在 parameters 中并通过 model.parameters() 取出
- 一个 state_dict 是一个字典，它将每一层映射到该层对应的可学习参数张量，注意只有可学习参数的层才会被保存在字典中（如卷积层、线性层等）
- 优化器对象 torch.optim 也有 state_dict，其保存了优化器的状态，包括了使用的一些超参数
- 由于 state_dict 是字典对象，因此我们能很容易的保存、更新、修改和读取，以此来修改模型和优化器
- 我们首先创建一个模型，用于测试保存和加载：
```py
# 定义模型
class TheModelClass(nn.Module):
    def __init__(self):
        super(TheModelClass, self).__init__()
        # 定义各个层
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 组装各个层，前向传播
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 初始化模型
model = TheModelClass()

# 初始化优化器
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# 打印模型的 state_dict，其key和层的名字有关
print("Model's state_dict:")
for param_tensor in model.state_dict():
    print(param_tensor, "\t", model.state_dict()[param_tensor].size())

# 打印优化器的 state_dict
print("Optimizer's state_dict:")
for var_name in optimizer.state_dict():
    print(var_name, "\t", optimizer.state_dict()[var_name])
```

# 2. 保存和加载模型：用于预测

## 2.1 只保存 state_dict (推荐)

**保存**

```py
PATH = "./model.pt"
# 保存前打印一下模型状态字典，由于太长，只打印conv2的偏置
print(model.state_dict()["conv2.bias"])  
# 保存模型状态字典到指定的path
torch.save(model.state_dict(), PATH)  
```

**加载**

```py
# 创建模型
model = TheModelClass()  
state_dict = torch.load(PATH)  # 从文件读取保存的字典状态
print(state_dict["conv2.bias"])  # 打印读取到的模型状态字典，验证是否一致
model.load_state_dict(state_dict)  # 将状态字典载入到当前模型
model.eval()  # 告诉dropout和batch normalization这是测试模式
```

- 保存模型用于预测时，则只需保存可训练参数即可
- 通过 torch.save() 保存 state_dict 将具有更高的灵活性，这也是推荐的做法
- 常规的 PyTorch 保存的文件采用 .pt 或 .pth 作为后缀
- 记住，在执行测试前，还要执行 model.eval()，以设置 dropout 和 batch normalization 到测试模式来用于推断，忘记这不可能导致不一致的推断结果
- 注意，load_state_dict() 方法接受一个字典对象，而不是一个路径，这意味着，你在载入模型参数之前，需要将模型参数读取到一个 map

## 2.2 保存整个模型

**保存**

```py
PATH = "./entire_model.pt"
# 保存前打印一下模型状态字典，由于太长，只打印conv2的偏置
print(model.state_dict()["conv2.bias"])  
torch.save(model, PATH)  # 保存整个模型
```

**读取**

```py
# 模型类必须在某个地方定义，否则无法载入模型（显然，你都没有那个类）
model = torch.load(PATH)  # 读取保存的整个模型
print(model.state_dict()["conv2.bias"])  # 打印读取到的模型状态字典，验证是否一致
model.eval()  # 告诉dropout和batch normalization这是测试模式
```

- 该种方式使用最直观的语法和最少量的代码
- 使用这种方式会序列化整个模型，通过 pickle 实现
- 该方法的缺点是保存模型时，模型的序列化数据被绑定到指定的类和目录结构，因为 pickle 序列化时并不保存类本身，而是序列化了类的路径，这在加载时会用到（对 pickle 不熟悉，这里不理解），因此你保存的模型用到其他项目或者重命名之后，可能会导致出错
- 仍然以 .pt 或 .pth 作为文件扩展名
- 记住，在执行测试前，还要执行 model.eval()，以设置 dropout 和 batch normalization 到测试模式来用于推断，忘记这不可能导致不一致的推断结果

# 3. 保存和加载模型：常规检查点（可测试或继续训练）

- 训练到某一步，模型到达的状态，将其称之为常规检查点（general checkpoint）
- 我们保存常规检查点，希望能用于推断或继续训练，除了模型的 state_dict 外，还要保存其他一些额外的东西，包括：
    - 优化器的 state_dict : 其包含了一些用于模型训练更新的缓存和参数
    - epoch : 当前训练到的 epoch，保存下来用于确定还需继续训练多少个epoch
    - loss : 最后一次的训练 loss 往往也会存储
    - 还有其他如 torch.nn.Embedding, layers 等等
- 为了保存多个组件，一般会将他们组织成一个字典，探后调用 torch.save() 存储到一个文件
- 通常存储了多个 state_dict 的 checkpoint 使用 .tar 的扩展名
- 载入模型时，首先创建 model 和 optimizer 对象，然后使用 torch.load() 载入 checkpoint，然后取出其各个项，赋值给不同的模型或者 Tensor
- 最后记得，如果是要执行测试，还要执行以下 model.eval() 来告诉模型接下来是要做测试，以避免 dropout 和 batch normalization 的不一致性
- 若是为了继续训练，则执行 model.train()，以告诉模型接下来是要继续训练，dropout 和 batch normalization 按训练模式运行
- 保存模型可以使用这样的代码：
```py
torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            ...
            }, PATH)
```
- 加载模型可以使用这样的代码：
```py
model = TheModelClass(*args, **kwargs)
optimizer = TheOptimizerClass(*args, **kwargs)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

model.eval()
# - or -
model.train()
```

**代码详细**

- 官方的 tutorial 只提供了代码框架，没有可以执行和验证的代码，下面为按照自己理解编写的代码
- 要保存训练到的 epoch、loss 等，首先必须要模拟训练过程，而要训练必须要先准备数据，我使用 CIFAR10 数据集作为训练集，相关教程可以查看官网或相关笔记
```py
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader

# 准备数据
com_transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])

trainset = torchvision.datasets.CIFAR10(root="./data", train=True,
                                      download=False, transform=com_transform)
trainloader = DataLoader(trainset, batch_size=10, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root="./data", train=False,
                                     download=False, transform=com_transform)
testloader = DataLoader(testset, batch_size=10, shuffle=False, num_workers=0)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

```
- 有了数据后，则利用本笔记前面定义的神经网络进行训练，并在训练到第2个epoch时结束训练，跳出循环
```py
num_epochs = 5
PATH = "./checkpoint.tar"
# 若电脑不好可能花费较多时间，这里没有采用 GPU
for epoch in range(num_epochs):
    epoch_loss = 0.0
    for step, data in enumerate(trainloader):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        epoch_loss += loss
        loss.backward()
        optimizer.step()
    
    epoch_loss /= step  # 计算本次epoch的平均loss
    print("average loss of epoch %d : %f" %(epoch, epoch_loss))
    
    if epoch == 2:  # 先训练3个epoch，跳出循环
        # 保存前先打印部分信息，用于校验
        print("epoch:", epoch)
        print("conv2.bias:", model.state_dict()["conv2.bias"])
        print("optimizer_state_dict: lr=%f, momentum=%f:" % (
            optimizer.state_dict()["param_groups"][0]["lr"],
            optimizer.state_dict()["param_groups"][0]["momentum"]))
        print("epoch_loss:",epoch_loss)
        
        # 保存模型，跳出循环，以后继续训练
        torch.save({
            "epoch": epoch, # 当前训练到第几个epoch
            "model_state_dict" : model.state_dict(),  # 模型状态字典
            "optimizer_state_dict" : optimizer.state_dict(),  # 优化器状态字典
            "epoch_loss" : epoch_loss  # 当前epoch的平均loss
        }, PATH)
        
        break  # 跳出循环，以后可以继续训练
```
- 我们已经有了一个训练了三个epoch的模型并保存到硬盘上，接下来我们载入模型：
```py
# 载入模型前，要先创建模型对象，这样才能容纳模型参数
model = TheModelClass()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)  # 故意定义一个不一样的超参数
criterion = nn.CrossEntropyLoss()  # 继续训练的损失函数要保持一致

# 载入 checkpoint 数据
checkpoint = torch.load(PATH)  

# 载入后，打印相关值，验证载入的数据和前面保存时打印的是否一致
print("epoch:", checkpoint['epoch'])
print("conv2.bias:", checkpoint['model_state_dict']["conv2.bias"])
print("optimizer_state_dict: lr=%f, momentum=%f:" % (
    checkpoint['optimizer_state_dict']["param_groups"][0]["lr"],
    checkpoint['optimizer_state_dict']["param_groups"][0]["momentum"]))
print("epoch_loss:",checkpoint['epoch_loss'])

# 将模型参数载入到model、optimizer中
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
epoch_loss = checkpoint['epoch_loss']

# model.eval()  # 若是要用于测试，则执行这个
# - or -
model.train()  # 由于要继续训练，则实行这个
```
- 根据载入的模型参数，继续剩余未完成的训练
```py
# 根据载入的 epoch、model、optimizer 继续剩余的训练，当然模型结构和损失函数不能更改
current_epoch = epoch + 1 
for epoch in range(current_epoch, num_epochs):  # 继续训练剩余的两个 epoch
    epoch_loss = 0.0
    for step, data in enumerate(trainloader):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        epoch_loss += loss
        loss.backward()
        optimizer.step()
    epoch_loss /= step
    
    # 为了方便，这里打印模型即可，不再保存模型了
    print("average loss of epoch %d : %f" %(epoch, epoch_loss))
    
# 若需要，在最后可保存最终的模型结果，保存方法和前面一致

```

# 4. 在单个文件中保存多个模型

- 在单文件中保存多个模型和前面类似，只要将多个模型的参数整合到一个字典然后用 torch.save() 存储即可
- 保存样例：
```py
torch.save({
            'modelA_state_dict': modelA.state_dict(),
            'modelB_state_dict': modelB.state_dict(),
            'optimizerA_state_dict': optimizerA.state_dict(),
            'optimizerB_state_dict': optimizerB.state_dict(),
            ...
            }, PATH)
```
- 载入样例：
```py
modelA = TheModelAClass(*args, **kwargs)
modelB = TheModelBClass(*args, **kwargs)
optimizerA = TheOptimizerAClass(*args, **kwargs)
optimizerB = TheOptimizerBClass(*args, **kwargs)

checkpoint = torch.load(PATH)
modelA.load_state_dict(checkpoint['modelA_state_dict'])
modelB.load_state_dict(checkpoint['modelB_state_dict'])
optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])
optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])

modelA.eval()
modelB.eval()
# - or -
modelA.train()
modelB.train()
```

# 5. 基于不同模型参数的热启动模型

- 有时，我们可能想使用迁移学习，将某个已经训练好的经典网络 A 作为我们自定义的新网络 B 的前半部分，此时可以载入这些部分层的参数
- 保存：
```py
torch.save(modelA.state_dict(), PATH)
```
- 载入：
```py
modelB = TheModelBClass(*args, **kwargs)
modelB.load_state_dict(torch.load(PATH), strict=False)
```
- 载入部分模型在迁移学习或从头训练一个复杂网络时非常常用
- 利用已训练的参数，即使是少数可用，也有助于热启动训练过程，并能帮助你的模型比从头开始更快的收敛
- 只要设置 strict=False，则载入模型状态时，不关字典中的键值对更多或更少，都没有关系，框架会忽视不匹配的键值对，而载入匹配的名称的键值对
- 若你想将某层的参数载入到另一层，但键不匹配，此时只需修改字典的 key 的名称，匹配到你想载入到的那一个层的名称即可

# 6. 在不同的设备上保存和加载模型

## 6.1 训练并保存为 GPU 模型， 读取为 CPU 模型

- 保存：
```py
torch.save(model.state_dict(), PATH)
```
- 载入：
```py
device = torch.device('cpu')
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))
```
- 要在 CPU 上载入在 GPU 上训练并保存的模型，只需在载入时提供将设备对象参数，即 torch.load() 的 map_location 参数


## 6.2 训练并保存为 GPU 模型， 读取为 GPU 模型

- 保存：
```py
torch.save(model.state_dict(), PATH)
```
- 载入：
```py
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.to(device)
# Make sure to call input = input.to(device) on any input tensors that you feed to the model
```
- 对于在 GPU 上训练并保存的模型，只需对模型执行 `model.to(torch.device('cuda'))` 即可
- 请务必确认输入数据也转换到 GPU 上，`my_tensor.to(device)` 会返回数据在 GPU 上的拷贝，不会覆盖原有的 my_tensor
- 因此，记得对于张量，需要执行赋值： `my_tensor = my_tensor.to(torch.device('cuda'))`，而模型不必赋值，只需执行 to 方法即可

## 6.3 训练并保存为 CPU 模型， 读取为 GPU 模型

- 存储：
```py
torch.save(model.state_dict(), PATH)
```
- 载入：
```py
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location="cuda:0"))  # Choose whatever GPU device number you want
model.to(device)
# Make sure to call input = input.to(device) on any input tensors that you feed to the model
```

- 要在 GPU 上载入在 CPU 上训练并保存的模型，则需要将设备提供给 torch.load() 函数的 map_location 参数
- 之后，还需要执行一次 `model.to(torch.device('cuda'))` 来转换模型到 GPU
- 最后，确认输入数据也转换到 GPU 上：`my_tensor = my_tensor.to(torch.device('cuda'))`

## 6.4 保存 torch.nn.DataParallel (并行计算模型)

- 保存：
```py
torch.save(model.module.state_dict(), PATH)
```
- torch.nn.DataParallel is a model wrapper that enables parallel GPU utilization. To save a DataParallel model generically, save the model.module.state_dict(). This way, you have the flexibility to load the model any way you want to any device you want.